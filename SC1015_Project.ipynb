{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chezyio/SC1015_Project/blob/main/SC1015_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU2BX4AFdneO"
      },
      "source": [
        "<h1>Convolutional Neural Network (CNN) Architecture</h1>\n",
        "\n",
        "<p>Convolutional Neural Networks (CNNs) are a type of neural network that are specifically designed for processing data that has a grid-like structure, such as images, video, and audio. CNNs are commonly used for image classification, object detection, and segmentation tasks.\n",
        "\n",
        "CNNs consist of several layers of neurons, each of which performs a specific function. The three main types of layers in a CNN are:\n",
        "\n",
        "\n",
        "Convolutional Layers: These layers apply a set of learnable filters to the input data to extract features that are important for the task at hand. Each filter slides over the input data and performs element-wise multiplication followed by summation to produce a single value, which is then passed through an activation function. The output of a convolutional layer is a set of feature maps that represent different aspects of the input data.\n",
        "\n",
        "\n",
        "Pooling Layers: These layers downsample the feature maps by taking the maximum, average, or other function of a local region. This reduces the dimensionality of the feature maps while retaining the important information.\n",
        "\n",
        "\n",
        "\n",
        "Fully Connected Layers: These layers perform a classification or regression task on the features extracted by the previous layers. Each neuron in a fully connected layer is connected to all the neurons in the previous layer.\n",
        "\n",
        "\n",
        "\n",
        "CNNs are trained using backpropagation and stochastic gradient descent to minimize a loss function. During training, the weights of the filters and the fully connected layers are updated iteratively to improve the network's performance.\n",
        "</p>\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vkQ0hXDaQv57sALXAJquxA.jpeg'>\n",
        "\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1052/1*GcI7G-JLAQiEoCON7xFbhg.gif'>\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1192/format:webp/1*KQIEqhxzICU7thjaQBfPBQ.png'>\n",
        "\n",
        "<div>\n",
        "  <h3>Workflow</h3>\n",
        "  <ol>\n",
        "    <li>Examine and understand data</li>\n",
        "    <li>Build input pipeline</li>\n",
        "    <li>Build model</li>\n",
        "    <li>Train model</li>\n",
        "    <li>Test model</li>\n",
        "    <li>Tune hyperparameters and iterate</li>\n",
        "  </ol>\n",
        "</div>\n",
        "<div>\n",
        "  <h3>Technical Specifications</h3>\n",
        "\n",
        "  // EDIT IN PROGRESS\n",
        "\n",
        "  <p>2 convolutions with max pool after each convolution, use 2 fc to make feature map, 2nd fc has 1 unit becasue it needs to predict if data has pneumonia or not\n",
        "  \n",
        "  \n",
        "  </p>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUXBDFCFlXsc"
      },
      "source": [
        "<h1>Preamble</h1>\n",
        "\n",
        " \n",
        "<p>Pneumonia is an infection that inflames the air sacs in both lungs. It is one of the leading causes of death in Singapore and worldwide. Accounting for 20.7%, 18.8% and 18.4% of deaths in Singapore in 2019,2020 and 2021 according to the death statistics retrieve from HealthHub. As for worldwide, statistics has shown that 2.5 million people have died from pneumonia in 2019.</p> \n",
        "<br>\n",
        "<p>Pneumonia can be caused by viral, bacterial and fungi. Common pneumonias are contagious and can be spread from person to person or through the contact with surfaces or objects that are contaminated by the bacteria or viruses. One example of a viral infections that can cause pneumonia which is common now is the coronavirus infection </p>\n",
        "<br>\n",
        "<p> However upon proper detection and treatment, many cases of pneumonia can be cleared without complications. One of the effective ways to identify signs of an inflammation will be Chest X-ray. Through x-rays, doctors will also be able to know the location and extent of this inflammation. </p>\n",
        "<br>\n",
        "<p> For the treatment of mild Pneumonia, it can be easily done through antibiotic, antiviral or antifungal medications.  In the case of any underlining health issues, one might be hospitalized and receive treatments such as respiratory and oxygen therapy and will be required to be injected with antibiotics.  </p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "u1zsaIaLaoQS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRkPlwz8kijG"
      },
      "source": [
        "<h1>Import Libraries</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "wKUofvQoj9zK"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Rescaling, Dropout\n",
        "from keras.models import Sequential, load_model\n",
        "import cv2\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RAo8itHh9W2R"
      },
      "outputs": [],
      "source": [
        "# Kaggle API Key\n",
        "os.environ['KAGGLE_USERNAME'] = \"territellis\"\n",
        "os.environ['KAGGLE_KEY'] = \"c9fcf5c0a806a25124e165d1530b3d75\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5Fpu6QYpLYT",
        "outputId": "5daeca6e-4c97-43b0-899e-1ee11c19ae98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chest-xray-pneumonia.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download paultimothymooney/chest-xray-pneumonia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPiin1D7qxl8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip chest-xray-pneumonia.zip -d chest_xray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZWrWBJUkl7d"
      },
      "source": [
        "<h1>Exploratory Data Analysis</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-M8pw9lkuPQ"
      },
      "outputs": [],
      "source": [
        "train = './chest_xray/chest_xray/train/'\n",
        "test = './chest_xray/chest_xray/test/'\n",
        "val = './chest_xray/chest_xray/val/'\n",
        "\n",
        "trainN = train + 'NORMAL/'\n",
        "trainP = train + 'PNEUMONIA/'\n",
        "\n",
        "testN = test + 'NORMAL/'\n",
        "testP = test + 'PNEUMONIA/'\n",
        "\n",
        "valN = val + 'NORMAL/'\n",
        "valP = val + 'PNEUMONIA/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNHrvzBkWaGF"
      },
      "outputs": [],
      "source": [
        "rand_norm = np.random.randint(0,len(os.listdir(trainN)))\n",
        "norm_pic = os.listdir(trainN)[rand_norm]\n",
        "print('normal picture title: ', norm_pic)\n",
        "\n",
        "norm_pic_add = trainN+norm_pic\n",
        "\n",
        "rand_p = np.random.randint(0, len(os.listdir(trainP)))\n",
        "\n",
        "pneu_pic = os.listdir(trainP)[rand_p]\n",
        "pneu_pic_add = trainP + pneu_pic\n",
        "print('pneumonia picture title: ', pneu_pic)\n",
        "\n",
        "# Load images\n",
        "norm_load = Image.open(norm_pic_add)\n",
        "pneu_load = Image.open(pneu_pic_add)\n",
        "\n",
        "f = plt.figure(figsize = (10,6))\n",
        "a1 = f.add_subplot(1,2,1)\n",
        "img_plot = plt.imshow(norm_load, cmap='gray')\n",
        "a1.set_title('Normal')\n",
        "\n",
        "f2 = plt.figure(figsize =(10,6))\n",
        "a2 = f2.add_subplot(1,2,1)\n",
        "img_plot2 = plt.imshow(pneu_load, cmap='gray')\n",
        "a2.set_title('Pneumonia')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6ExhF4Ga--K"
      },
      "outputs": [],
      "source": [
        "sns.distplot(norm_load.ravel(),\n",
        "             label=f\"Pixel Mean {np.mean(norm_load):.3f} & Standard Deviation {np.std(norm_load):.3f}\", \n",
        "             kde=False)\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Distribution of Pixel Intensities in the Image')\n",
        "plt.xlabel('Pixel Intensity')\n",
        "plt.ylabel('# Pixels in Image')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn6Me4VjfREp"
      },
      "outputs": [],
      "source": [
        "n_count = len(os.listdir(trainN))\n",
        "p_count = len(os.listdir(trainP))\n",
        "\n",
        "print(\"There are {} images in the training dataset\".format(len(os.listdir(trainN) + os.listdir(trainP))))\n",
        "print(\"There are {} images in the test dataset\".format(len(os.listdir(testN) + os.listdir(testP))))\n",
        "print(\"There are {} images in the validation dataset\".format(len(os.listdir(valN) + os.listdir(valP))))\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'Types': ['Normal', 'Pneumonia'], 'count':[n_count, p_count]})\n",
        "sns.barplot(x=df['count'], y=df['Types'], orient='h')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5kczGg0kusa"
      },
      "source": [
        "<h1>Image Augmentation and Processing</h1>\n",
        "\n",
        "<div>\n",
        "  Notes:\n",
        "  Can try to augment scale, gaussian blur, etc\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WssCm7m2e6eU"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "img_height = 64\n",
        "img_width = 64\n",
        "\n",
        "# tf.keras.utils.image_dataset_from_directory utility will take from a directory of images on disk to a tf.data.Dataset\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  train,\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "# used to evaluate accuracy of model\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  test,\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  val,\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print(class_names)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "# The image_batch is a tensor of the shape (32, 64, 64, 3). This is a batch of 32 images of shape 64x64x3 (the last dimension refers to color channels RGB). The label_batch is a tensor of the shape (32,), these are corresponding labels to the 32 images.\n",
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Train Model</h1>"
      ],
      "metadata": {
        "id": "5ZkrxLILgy6e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wasjI3uVk3eZ"
      },
      "outputs": [],
      "source": [
        "# 2 convolutions with max pool after each convolution\n",
        "# use 2 fc to make feature map, 2nd fc has 1 unit becasue it needs to predict if data has pneumonia or not\n",
        "\n",
        "# relu = rectified linear, values would range from 0 to infinity. any negative values would be treated as 0\n",
        "# sigmoid = values would range from 0 to 1\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# RandomFlip(\"horizontal\", input_shape=(img_height,img_width,3)),\n",
        "# RandomRotation(0.1),\n",
        "# RandomZoom(0.1),\n",
        "# Rescaling(1./255, input_shape=(img_height, img_width, 3)),  # [0,1] range\n",
        "model = Sequential([\n",
        "  Conv2D(32, (3, 3), activation=\"relu\", input_shape=(64, 64, 3)),\n",
        "  MaxPooling2D(pool_size = (2, 2)),\n",
        "  Dropout(0.2),\n",
        "  Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "  MaxPooling2D(pool_size = (2, 2)),\n",
        "  Dropout(0.2),\n",
        "  Conv2D(128, (3, 3), activation=\"relu\"),\n",
        "  MaxPooling2D(pool_size = (2, 2)),\n",
        "  Dropout(0.2),\n",
        "  Flatten(),\n",
        "  Dense(activation = 'relu', units = 128),\n",
        "  Dense(num_classes)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Brfa-0FJkyQA"
      },
      "outputs": [],
      "source": [
        "# train model\n",
        "epochs=10\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1ZqjX3LkysI"
      },
      "source": [
        "<h1>Evaluate Model</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zogEpYwXdU9Y"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLAu_igcdYLH"
      },
      "outputs": [],
      "source": [
        "#test_path = val + 'NORMAL/NORMAL2-IM-1438-0001.jpeg'\n",
        "test_path = val + 'PNEUMONIA/person1949_bacteria_4880.jpeg'\n",
        "\n",
        "img = tf.keras.utils.load_img(\n",
        "    test_path, target_size=(img_height, img_width)\n",
        ")\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "print(\"This image most likely belongs to {} with a {:.2f} percent confidence.\".format(class_names[np.argmax(score)], 100 * np.max(score)))\n",
        "\n",
        "\n",
        "loss, acc = model.evaluate(test_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Val Normal Dataset\n",
        "nVal = [x for x in os.listdir(val + 'NORMAL/')]\n",
        "count = 1\n",
        "\n",
        "label = []\n",
        "for image in nVal:\n",
        "  imgFilePath = val + 'NORMAL/' + image\n",
        "  print(\"\\nIMAGE \" + str(count) + \": \" + image)\n",
        "  img = tf.keras.utils.load_img(\n",
        "    imgFilePath, target_size=(img_height, img_width)\n",
        "  )\n",
        "  img_array = tf.keras.utils.img_to_array(img)\n",
        "  img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "  predictions = model.predict(img_array)\n",
        "  score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "  lbl2 = []\n",
        "  lbl2.append(class_names[np.argmax(score)])\n",
        "  lbl2.append(100 * np.max(score))\n",
        "  label.append(lbl2)\n",
        "  #print(\"This image most likely belongs to {} with a {:.2f} percent confidence.\".format(class_names[np.argmax(score)], 100 * np.max(score)))\n",
        "  count+=1\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(len(nVal)):\n",
        "  loadImg = Image.open(val + 'NORMAL/' + nVal[i])\n",
        "  ax = plt.subplot(3, 3, i+1)\n",
        "  plt.imshow(loadImg, cmap='gray')\n",
        "  plt.title(\"{} \\nconfidence: {:.2f}\" .format(label[i][0], label[i][1]))\n",
        "  plt.axis(\"off\")\n",
        "    "
      ],
      "metadata": {
        "id": "bOA7m6Mbrvpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(len(nVal)):\n",
        "  img = cv2.imread(val + 'NORMAL/' + nVal[i])\n",
        "  img = cv2.resize(img, (512, 512))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "  img = cv2.addWeighted (img, 4, cv2.GaussianBlur(img, (0, 0), 512/10), -4, 128)\n",
        "  ax = plt.subplot(3, 3, i+1)\n",
        "  plt.imshow(img)\n",
        "  plt.title(\"{} \\nconfidence: {:.2f}\" .format(label[i][0], label[i][1]))\n",
        "  plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "qNhHd0BArVaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(len(nVal)):\n",
        "  img = cv2.imread(val + 'NORMAL/' + nVal[i])\n",
        "  img = cv2.resize(img, (512, 512))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "  detected_edges = cv2.Canny(img, 80, 100)\n",
        "  ax = plt.subplot(3, 3, i+1)\n",
        "  plt.imshow(detected_edges)\n",
        "  plt.title(\"{} \\nconfidence: {:.2f}\" .format(label[i][0], label[i][1]))\n",
        "  plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "Q7WTgkHPthuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Val Pneumonia Dataset\n",
        "pVal = [p for p in os.listdir(val + 'PNEUMONIA')]\n",
        "count = 1\n",
        "label = []\n",
        "for imagePath in pVal:\n",
        "  test_path = val + 'PNEUMONIA/' + imagePath\n",
        "  print(\"\\nIMAGE \" + str(count) + \": \" + image)\n",
        "  img = tf.keras.utils.load_img(\n",
        "    test_path, target_size=(img_height, img_width)\n",
        "  )\n",
        "  img_array = tf.keras.utils.img_to_array(img)\n",
        "  img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "  predictions = model.predict(img_array)\n",
        "  score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "  lbl2 = []\n",
        "  lbl2.append(class_names[np.argmax(score)])\n",
        "  lbl2.append(100 * np.max(score))\n",
        "  label.append(lbl2)\n",
        "  #print(\"This image most likely belongs to {} with a {:.2f} percent confidence.\".format(class_names[np.argmax(score)], 100 * np.max(score)))\n",
        "  count+=1\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(len(nVal)):\n",
        "  loadImg = Image.open(val + 'PNEUMONIA/' + pVal[i])\n",
        "  ax = plt.subplot(3, 3, i+1)\n",
        "  plt.imshow(loadImg, cmap='gray')\n",
        "  plt.title(\"{} \\nconfidence: {:.2f}\" .format(label[i][0], label[i][1]))\n",
        "  plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "ONIOk_q2sGNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D7vjW7LGztEY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}